{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import arviz as az\n",
    "import subprocess\n",
    "\n",
    "func_dict = {\"mean\": np.mean, \n",
    "             \"q2.5\": lambda x: np.percentile(x, 2.5), \n",
    "             \"q97.5\": lambda x: np.percentile(x, 97.5)}\n",
    "\n",
    "\n",
    "output_dir = \"../../results/Scenario-1/sens_WuhanPop\"\n",
    "#!rm -rf {output_dir}\n",
    "!mkdir -p {output_dir}\n",
    "output_data_dir = output_dir + \"/datasets\"\n",
    "!mkdir -p {output_data_dir}\n",
    "\n",
    "from scipy.integrate import quad\n",
    "\n",
    "class Integrate(theano.Op):\n",
    "    def __init__(self, expr, var, *extra_vars):\n",
    "        super().__init__()\n",
    "        self._expr = expr\n",
    "        self._var = var\n",
    "        self._extra_vars = extra_vars\n",
    "        self._func = theano.function(\n",
    "            [var] + list(extra_vars),\n",
    "            self._expr,\n",
    "            on_unused_input='ignore')\n",
    "    \n",
    "    def make_node(self, start, stop, *extra_vars):\n",
    "        self._extra_vars_node = extra_vars\n",
    "        assert len(self._extra_vars) == len(extra_vars)\n",
    "        self._start = start\n",
    "        self._stop = stop\n",
    "        vars = [start, stop] + list(extra_vars)\n",
    "        return theano.Apply(self, vars, [tt.dscalar().type()])\n",
    "    \n",
    "    def perform(self, node, inputs, out):\n",
    "        start, stop, *args = inputs\n",
    "        val = quad(self._func, start, stop, args=tuple(args))[0]\n",
    "        out[0][0] = np.array(val)\n",
    "        \n",
    "    def grad(self, inputs, grads):\n",
    "        start, stop, *args = inputs\n",
    "        out, = grads\n",
    "        replace = dict(zip(self._extra_vars, args))\n",
    "        \n",
    "        replace_ = replace.copy()\n",
    "        replace_[self._var] = start\n",
    "        dstart = out * theano.clone(-self._expr, replace=replace_)\n",
    "        \n",
    "        replace_ = replace.copy()\n",
    "        replace_[self._var] = stop\n",
    "        dstop = out * theano.clone(self._expr, replace=replace_)\n",
    "\n",
    "        grads = tt.grad(self._expr, self._extra_vars)\n",
    "        dargs = []\n",
    "        for grad in grads:\n",
    "            integrate = Integrate(grad, self._var, *self._extra_vars)\n",
    "            darg = out * integrate(start, stop, *args)\n",
    "            dargs.append(darg)\n",
    "            \n",
    "        return [dstart, dstop] + dargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using advi...\n",
      "Average Loss = 155.63:   6%|▌         | 11725/200000 [00:03<00:53, 3523.11it/s]\n",
      "Convergence achieved at 12000\n",
      "Interrupted at 11,999 [5%]: Average Loss = 260.97\n",
      "Multiprocess sampling (5 chains in 5 jobs)\n",
      "NUTS: [b_delay, a_delay]\n",
      "Sampling 5 chains, 0 divergences: 100%|██████████| 125000/125000 [00:37<00:00, 3351.46draws/s]\n",
      "The number of effective samples is smaller than 25% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using advi...\n",
      "Average Loss = 2,020.9:   5%|▌         | 10999/200000 [01:19<22:43, 138.59it/s]  \n",
      "Convergence achieved at 11000\n",
      "Interrupted at 10,999 [5%]: Average Loss = 4.4456e+09\n",
      "Multiprocess sampling (10 chains in 10 jobs)\n",
      "NUTS: [neglogq, neglogr]\n",
      "Sampling 10 chains, 0 divergences: 100%|██████████| 65000/65000 [49:42<00:00, 21.80draws/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using advi...\n",
      "Average Loss = 159.05:   6%|▌         | 11131/200000 [00:03<00:55, 3422.10it/s]\n",
      "Convergence achieved at 11200\n",
      "Interrupted at 11,199 [5%]: Average Loss = 272.36\n",
      "Multiprocess sampling (5 chains in 5 jobs)\n",
      "NUTS: [b_delay, a_delay]\n",
      "Sampling 5 chains, 0 divergences: 100%|██████████| 125000/125000 [00:30<00:00, 4152.51draws/s]\n",
      "The number of effective samples is smaller than 25% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using advi...\n",
      "Average Loss = 421.66:   7%|▋         | 13685/200000 [01:35<21:39, 143.33it/s]   \n",
      "Convergence achieved at 13700\n",
      "Interrupted at 13,699 [6%]: Average Loss = 1.1278e+10\n",
      "Multiprocess sampling (10 chains in 10 jobs)\n",
      "NUTS: [neglogq, neglogr]\n",
      "Sampling 10 chains, 0 divergences: 100%|██████████| 65000/65000 [47:34<00:00, 22.77draws/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20min 48s, sys: 24.4 s, total: 21min 12s\n",
      "Wall time: 1h 42min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!rm -rf data_tmp_WuhanPop_1\n",
    "!mkdir -p data_tmp_WuhanPop_1\n",
    "\n",
    "start = theano.shared(0.)\n",
    "stop = theano.shared(250.)\n",
    "μ = theano.shared(2.838)\n",
    "σ = theano.shared(0.520)\n",
    "\n",
    "for idx, WuhanPop in enumerate(['15000000', '19000000']): #baseline '11081000' considered in the main text\n",
    "    t0 = '2019-12-08'\n",
    "    CUTOFF_TIME = '2020-01-24'\n",
    "    Tinspection = '12.5'\n",
    "    \n",
    "    print(WuhanPop)\n",
    "    subprocess.call(['Rscript', 'prepare_data_addtl_argvs.R', './data_tmp_WuhanPop_1', t0, CUTOFF_TIME, WuhanPop, Tinspection])\n",
    "    \n",
    "    df = pd.read_csv(\"data_tmp_WuhanPop_1/data.csv\")\n",
    "    df_onset2death = pd.read_csv(\"data_tmp_WuhanPop_1/data_onset2death.csv\")\n",
    "    df_onset2report = pd.read_csv(\"data_tmp_WuhanPop_1/data_onset2report.csv\")\n",
    "\n",
    "    for idx0, flnm in enumerate(['data.csv', 'data_onset2death.csv', 'data_onset2report.csv']):\n",
    "        !cp data_tmp_WuhanPop_1/{flnm} {output_data_dir}/{CUTOFF_TIME}_{flnm}\n",
    "            \n",
    "    # module for onset2report\n",
    "    with pm.Model() as model_reporting_delay:\n",
    "        a_delay = pm.HalfNormal('a_delay', sd=5)\n",
    "        b_delay = pm.HalfCauchy('b_delay', 2.5)\n",
    "        timeOnsetToDeath = df_onset2report.dist.values\n",
    "        pm.Gamma('likelihood_delay', a_delay, b_delay, observed=timeOnsetToDeath)\n",
    "        pm.Deterministic('mean_delay', a_delay/b_delay);\n",
    "        pm.Deterministic('sd_delay', np.sqrt(a_delay)/b_delay);\n",
    "        trace_reporting_delay = pm.sample(20000, tune=5000, cores=5, target_accept=.85, init='advi')\n",
    "\n",
    "    res_delay = pm.summary(trace_reporting_delay, var_names=['a_delay', 'b_delay', 'mean_delay'])['mean']\n",
    "    df_res = az.summary(trace_reporting_delay, var_names=['mean_delay', 'sd_delay', 'a_delay', 'b_delay'], stat_funcs=func_dict, extend=False, round_to=5).reset_index().rename(columns={'index': 'var'})\n",
    "    df_res.rename(columns={'q2.5': 'lower', 'q97.5': 'upper'}).loc[:,['var','mean','lower','upper']].\\\n",
    "        to_csv(output_dir+'/'+CUTOFF_TIME+'_onset2report.csv', index=False)\n",
    "\n",
    "    # main module\n",
    "    inci_idx = np.min(df.loc[lambda d: d.exports>0].index)\n",
    "    inci_tmin = df.loc[inci_idx,'time']\n",
    "    len_p = len(df.loc[lambda d: d['time']>=inci_tmin,'prob_travel'])\n",
    "    death_idx = np.min(df.loc[lambda d: d['deaths']>0].index)\n",
    "    with pm.Model() as model:  \n",
    "        ## main data and priors ##\n",
    "        K = df['exports'].shape[0]\n",
    "        exported_cases = df['exports'].values\n",
    "        p = df.loc[0,'prob_travel']\n",
    "\n",
    "        neglogr = pm.HalfNormal('neglogr', testval=-np.log(0.1))\n",
    "        r = pm.Deterministic('r',np.exp(-neglogr))\n",
    "        i0 = 1.0\n",
    "\n",
    "        t = tt.arange(1,K+1,1)\n",
    "        Incidence = pm.Deterministic('Incidence',i0*(np.exp(r*t)-1.0)/r)\n",
    "\n",
    "        ## implementing numerical integration \n",
    "        s = tt.dscalar('s')\n",
    "        s.tag.test_value = np.zeros(()) #variable of integration\n",
    "        r_ = tt.dscalar('r_')\n",
    "        r_.tag.test_value = np.ones(())*0.14\n",
    "        func = tt.exp(-r_*s)/s/σ/((2.0*np.pi)**0.5)*tt.exp(-((tt.log(s)-μ)**2)/2/(σ**2))\n",
    "        integrate = Integrate(func, s, r_)\n",
    "\n",
    "        ## calculating us ##\n",
    "        u_delay = pm.Deterministic('u_delay', (1 + r*res_delay['mean_delay']/res_delay['a_delay'])**(-res_delay['a_delay']))\n",
    "        u_death = pm.Deterministic('u_death', integrate(start, stop, r))\n",
    "        ##############################\n",
    "\n",
    "        ## reconstructed incidence from exportation events ##\n",
    "        mu = (u_delay*Incidence*p/(1-p))[inci_idx:K]\n",
    "        alpha = (1.0/(1-p))\n",
    "        pm.Gamma('likelihood_incidence', mu, alpha, shape=K-death_idx, observed=exported_cases[inci_idx:K])\n",
    "        ##############################\n",
    "\n",
    "        ## CFR ##\n",
    "        death = df['deaths'].values\n",
    "        neglogq = pm.Gamma('neglogq', 2, .5, shape=K-death_idx, testval=-np.log(.06))\n",
    "        q = pm.Deterministic('q',np.exp(-neglogq))\n",
    "\n",
    "        shape_death = u_death*Incidence[death_idx:K]*q/(1-q)\n",
    "        invscale_death = 1.0/(1-q)\n",
    "        pm.Gamma('likelihood_death', shape_death, invscale_death, observed=death[death_idx:K])\n",
    "        ##############################\n",
    "\n",
    "        pm.Deterministic('predictedDeath', u_death*Incidence[death_idx:K]*q)\n",
    "\n",
    "        sample = pm.sample(4000, cores=10, tune=2500, target_accept=.92, init='advi')\n",
    "\n",
    "    df_res = az.summary(sample, \n",
    "                        var_names=['r','Incidence','q','u_delay','predictedDeath'], \n",
    "                        stat_funcs=func_dict, extend=False, round_to=6).reset_index().rename(columns={'index': 'var'})\n",
    "    df_res['time'] = df_res['var'].apply(lambda st: st[st.find(\"[\")+1:st.find(\"]\")])\n",
    "    df_res['time'] = ['NA' if \"[\" not in y else int(x)+1 for x,y in zip(df_res['time'],df_res['var'])]\n",
    "    df_res['var'] = df_res['var'].apply(lambda st: st[:st.find(\"[\")] if \"[\" in st else st)\n",
    "    df_res.loc[lambda d: d['var']=='q', 'var'] = 'CFR'\n",
    "    df_res.rename(columns={'q2.5': 'lower', 'q97.5': 'upper'}).loc[:,['var','time','mean','lower','upper']].\\\n",
    "        to_csv(output_dir+'/'+WuhanPop+'_incidence.csv', index=False)\n",
    "    \n",
    "!rm -rf data_tmp_WuhanPop_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
