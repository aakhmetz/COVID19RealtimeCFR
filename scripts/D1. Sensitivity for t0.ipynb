{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import arviz as az\n",
    "import subprocess\n",
    "\n",
    "func_dict = {\"mean\": np.mean, \n",
    "             \"q2.5\": lambda x: np.percentile(x, 2.5), \n",
    "             \"q97.5\": lambda x: np.percentile(x, 97.5)}\n",
    "\n",
    "output_dir = \"../../results/sens_t0\"\n",
    "# !rm -rf {output_dir}\n",
    "!mkdir -p {output_dir}\n",
    "output_data_dir = output_dir + \"/datasets\"\n",
    "!mkdir -p {output_data_dir}\n",
    "\n",
    "\n",
    "from scipy.integrate import quad\n",
    "\n",
    "class Integrate(theano.Op):\n",
    "    def __init__(self, expr, var, *extra_vars):\n",
    "        super().__init__()\n",
    "        self._expr = expr\n",
    "        self._var = var\n",
    "        self._extra_vars = extra_vars\n",
    "        self._func = theano.function(\n",
    "            [var] + list(extra_vars),\n",
    "            self._expr,\n",
    "            on_unused_input='ignore')\n",
    "    \n",
    "    def make_node(self, start, stop, *extra_vars):\n",
    "        self._extra_vars_node = extra_vars\n",
    "        assert len(self._extra_vars) == len(extra_vars)\n",
    "        self._start = start\n",
    "        self._stop = stop\n",
    "        vars = [start, stop] + list(extra_vars)\n",
    "        return theano.Apply(self, vars, [tt.dscalar().type()])\n",
    "    \n",
    "    def perform(self, node, inputs, out):\n",
    "        start, stop, *args = inputs\n",
    "        val = quad(self._func, start, stop, args=tuple(args))[0]\n",
    "        out[0][0] = np.array(val)\n",
    "        \n",
    "    def grad(self, inputs, grads):\n",
    "        start, stop, *args = inputs\n",
    "        out, = grads\n",
    "        replace = dict(zip(self._extra_vars, args))\n",
    "        \n",
    "        replace_ = replace.copy()\n",
    "        replace_[self._var] = start\n",
    "        dstart = out * theano.clone(-self._expr, replace=replace_)\n",
    "        \n",
    "        replace_ = replace.copy()\n",
    "        replace_[self._var] = stop\n",
    "        dstop = out * theano.clone(self._expr, replace=replace_)\n",
    "\n",
    "        grads = tt.grad(self._expr, self._extra_vars)\n",
    "        dargs = []\n",
    "        for grad in grads:\n",
    "            integrate = Integrate(grad, self._var, *self._extra_vars)\n",
    "            darg = out * integrate(start, stop, *args)\n",
    "            dargs.append(darg)\n",
    "            \n",
    "        return [dstart, dstop] + dargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using advi...\n",
      "Average Loss = 162.42:   6%|▌         | 11226/200000 [00:02<00:49, 3815.16it/s]\n",
      "Convergence achieved at 11500\n",
      "Interrupted at 11,499 [5%]: Average Loss = 272.93\n",
      "Multiprocess sampling (10 chains in 10 jobs)\n",
      "NUTS: [b_delay, a_delay]\n",
      "Sampling 10 chains, 0 divergences: 100%|██████████| 150000/150000 [00:33<00:00, 4513.46draws/s]\n",
      "The number of effective samples is smaller than 25% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using advi...\n",
      "Average Loss = 502.34:   5%|▌         | 10292/200000 [01:14<22:54, 138.00it/s]   \n",
      "Convergence achieved at 10300\n",
      "Interrupted at 10,299 [5%]: Average Loss = 6.1909e+09\n",
      "Multiprocess sampling (8 chains in 8 jobs)\n",
      "NUTS: [neglogq, neglogr]\n",
      "Sampling 8 chains, 0 divergences: 100%|██████████| 50000/50000 [52:31<00:00, 15.87draws/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 47s, sys: 10.5 s, total: 8min 57s\n",
      "Wall time: 54min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!rm -rf data_tmp\n",
    "!mkdir -p data_tmp\n",
    "\n",
    "start = theano.shared(0.)\n",
    "stop = theano.shared(250.)\n",
    "μ = theano.shared(2.838)\n",
    "σ = theano.shared(0.520)\n",
    "\n",
    "for idx, day0 in enumerate(range(10)):\n",
    "    t0 = '2019-12-%02d'%(day0+1)\n",
    "    CUTOFF_TIME = '2020-01-24'\n",
    "    \n",
    "    print(t0)\n",
    "    subprocess.call(['Rscript', 'prepare_data.R', './data_tmp', t0, CUTOFF_TIME])\n",
    "    \n",
    "    df = pd.read_csv(\"data_tmp/data.csv\")\n",
    "    df_onset2death = pd.read_csv(\"data_tmp/data_onset2death.csv\")\n",
    "    df_onset2report = pd.read_csv(\"data_tmp/data_onset2report.csv\")\n",
    "\n",
    "    for idx0, flnm in enumerate(['data.csv', 'data_onset2death.csv', 'data_onset2report.csv']):\n",
    "        if idx0==0:\n",
    "            !cp data_tmp/{flnm} {output_data_dir}/{t0}_{flnm}\n",
    "        else:\n",
    "            !cp data_tmp/{flnm} {output_data_dir}/{flnm}\n",
    "            \n",
    "    if (idx==0):\n",
    "        # module for onset2report\n",
    "        with pm.Model() as model_reporting_delay:\n",
    "            a_delay = pm.HalfNormal('a_delay', sd=5)\n",
    "            b_delay = pm.HalfCauchy('b_delay', 2.5)\n",
    "            timeOnsetToReport = df_onset2report.dist.values\n",
    "            pm.Gamma('likelihood_delay', a_delay, b_delay, observed=timeOnsetToReport)\n",
    "            pm.Deterministic('mean_delay', a_delay/b_delay);\n",
    "            pm.Deterministic('sd_delay', np.sqrt(a_delay)/b_delay);\n",
    "            trace_reporting_delay = pm.sample(10000, tune=5000, cores=10, target_accept=.85, init='advi')\n",
    "\n",
    "        res_delay = pm.summary(trace_reporting_delay, var_names=['a_delay', 'b_delay', 'mean_delay'])['mean']\n",
    "        df_res = az.summary(trace_reporting_delay, var_names=['mean_delay', 'sd_delay', 'a_delay', 'b_delay'], stat_funcs=func_dict, extend=False, round_to=5).reset_index().rename(columns={'index': 'var'})\n",
    "        df_res.rename(columns={'q2.5': 'lower', 'q97.5': 'upper'}).loc[:,['var','mean','lower','upper']].\\\n",
    "            to_csv(output_dir+'/onset2report.csv', index=False)\n",
    "\n",
    "    # main module\n",
    "    inci_idx = np.min(df.loc[lambda d: d.exports>0].index)\n",
    "    inci_tmin = df.loc[inci_idx,'time']\n",
    "    len_p = len(df.loc[lambda d: d['time']>=inci_tmin,'prob_travel'])\n",
    "    death_idx = np.min(df.loc[lambda d: d['deaths']>0].index)\n",
    "    with pm.Model() as model:  \n",
    "        ## main data and priors ##\n",
    "        K = df['exports'].shape[0]\n",
    "        exported_cases = df['exports'].values\n",
    "        p = df.loc[0,'prob_travel']\n",
    "\n",
    "        neglogr = pm.HalfNormal('neglogr', testval=-np.log(0.1))\n",
    "        r = pm.Deterministic('r',np.exp(-neglogr))\n",
    "        T0 = df['time'].values[inci_idx]\n",
    "        i0 = 1.0\n",
    "\n",
    "        t = tt.arange(1,K+1,1)\n",
    "        Incidence = pm.Deterministic('Incidence',i0*(tt.exp(r*t)-1.0)/r)\n",
    "\n",
    "        ## implementing numerical integration \n",
    "        s = tt.dscalar('s')\n",
    "        s.tag.test_value = np.zeros(()) #variable of integration\n",
    "        r_ = tt.dscalar('r_')\n",
    "        r_.tag.test_value = np.ones(())*0.14\n",
    "        func = tt.exp(-r_*s)/s/σ/((2.0*np.pi)**0.5)*tt.exp(-((tt.log(s)-μ)**2)/2/(σ**2))\n",
    "        integrate = Integrate(func, s, r_)\n",
    "\n",
    "        ## calculating us ##\n",
    "        u_delay = pm.Deterministic('u_delay', (1 + r*res_delay['mean_delay']/res_delay['a_delay'])**(-res_delay['a_delay']))\n",
    "        u_death = pm.Deterministic('u_death', integrate(start, stop, r))\n",
    "        ##############################\n",
    "\n",
    "        ## reconstructed incidence from exportation events ##\n",
    "        mu = (u_delay*Incidence*p/(1-p))[inci_idx:K]\n",
    "        alpha = (1.0/(1-p))\n",
    "        pm.Gamma('likelihood_incidence', mu, alpha, shape=K-death_idx, observed=exported_cases[inci_idx:K])\n",
    "        ##############################\n",
    "\n",
    "        ## CFR ##\n",
    "        death = df['deaths'].values\n",
    "        neglogq = pm.Gamma('neglogq', 2, .5, shape=K-death_idx, testval=-np.log(.06))\n",
    "        q = pm.Deterministic('q',np.exp(-neglogq))\n",
    "\n",
    "        shape_death = u_death*Incidence[death_idx:K]*q/(1-q)\n",
    "        invscale_death = 1.0/(1-q)\n",
    "        pm.Gamma('likelihood_death', shape_death, invscale_death, observed=death[death_idx:K])\n",
    "        ##############################\n",
    "\n",
    "        pm.Deterministic('predictedDeath', u_death*Incidence[death_idx:K]*q)\n",
    "\n",
    "        sample = pm.sample(3750, cores=8, tune=2500, target_accept=.92, init='advi')\n",
    "\n",
    "    df_res = az.summary(sample, \n",
    "                        var_names=['r', 'Incidence', 'q', 'u_delay', 'predictedDeath'], \n",
    "                        stat_funcs=func_dict, extend=False, round_to=6).reset_index().rename(columns={'index': 'var'})\n",
    "    df_res['time'] = df_res['var'].apply(lambda st: st[st.find(\"[\")+1:st.find(\"]\")])\n",
    "    df_res['time'] = ['NA' if \"[\" not in y else int(x)+1 for x,y in zip(df_res['time'],df_res['var'])]\n",
    "    df_res['var'] = df_res['var'].apply(lambda st: st[:st.find(\"[\")] if \"[\" in st else st)\n",
    "    df_res.loc[lambda d: d['var']=='q', 'var'] = 'CFR'\n",
    "    df_res.rename(columns={'q2.5': 'lower', 'q97.5': 'upper'}).loc[:,['var','time','mean','lower','upper']].\\\n",
    "        to_csv(output_dir+'/'+t0+'_incidence.csv', index=False)\n",
    "    \n",
    "!rm -rf data_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
